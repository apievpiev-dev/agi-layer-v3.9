#!/usr/bin/env python3
"""
Download Models Script for AGI Layer v3.9
==========================================

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π:
- LLM –º–æ–¥–µ–ª–∏ (Llama, Phi, Qwen)
- Vision –º–æ–¥–µ–ª–∏ (BLIP2, CLIP)
- Image Generation (Stable Diffusion)
- Audio –º–æ–¥–µ–ª–∏ (Whisper, Silero)
- Embedding –º–æ–¥–µ–ª–∏ (SentenceTransformers)
"""

import asyncio
import os
import sys
from pathlib import Path

import torch
from huggingface_hub import snapshot_download
from loguru import logger


class ModelDownloader:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.models_dir = Path("/app/models")
        self.cache_dir = self.models_dir / "cache"
        self.downloads_dir = self.models_dir / "downloads"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.models_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)
        self.downloads_dir.mkdir(exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logger.add(
            "/app/logs/model_download.log",
            rotation="10 MB",
            level="INFO"
        )

    async def download_all_models(self):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        models_to_download = [
            # LLM –º–æ–¥–µ–ª–∏
            {
                "name": "microsoft/Phi-3-mini-4k-instruct",
                "type": "llm",
                "priority": 1,
                "size_gb": 7
            },
            {
                "name": "meta-llama/Llama-3.2-3B-Instruct",
                "type": "llm", 
                "priority": 2,
                "size_gb": 6
            },
            {
                "name": "Qwen/Qwen2.5-7B-Instruct",
                "type": "llm",
                "priority": 3,
                "size_gb": 14
            },
            
            # Vision –º–æ–¥–µ–ª–∏
            {
                "name": "Salesforce/blip2-opt-2.7b",
                "type": "vision",
                "priority": 1,
                "size_gb": 5
            },
            {
                "name": "openai/clip-vit-base-patch32",
                "type": "vision",
                "priority": 2,
                "size_gb": 1
            },
            
            # Image Generation
            {
                "name": "runwayml/stable-diffusion-v1-5",
                "type": "image_gen",
                "priority": 1,
                "size_gb": 4
            },
            
            # Audio –º–æ–¥–µ–ª–∏
            {
                "name": "openai/whisper-base",
                "type": "stt",
                "priority": 1,
                "size_gb": 1
            },
            {
                "name": "microsoft/speecht5_tts",
                "type": "tts",
                "priority": 2,
                "size_gb": 1
            },
            
            # Embedding –º–æ–¥–µ–ª–∏
            {
                "name": "sentence-transformers/all-MiniLM-L6-v2",
                "type": "embeddings",
                "priority": 1,
                "size_gb": 0.5
            },
            {
                "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "type": "embeddings",
                "priority": 2,
                "size_gb": 1
            }
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        models_to_download.sort(key=lambda x: x["priority"])
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ {len(models_to_download)} –º–æ–¥–µ–ª–µ–π")
        
        total_size = sum(model["size_gb"] for model in models_to_download)
        logger.info(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: ~{total_size}GB")
        
        downloaded_count = 0
        
        for model_info in models_to_download:
            try:
                success = await self._download_single_model(model_info)
                if success:
                    downloaded_count += 1
                    logger.info(f"‚úÖ –°–∫–∞—á–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {downloaded_count}/{len(models_to_download)}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_info['name']}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {model_info['name']}: {e}")
                continue
        
        logger.info(f"üéâ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –£—Å–ø–µ—à–Ω–æ: {downloaded_count}/{len(models_to_download)}")

    async def _download_single_model(self, model_info: Dict) -> bool:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            model_name = model_info["name"]
            model_type = model_info["type"]
            
            logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º {model_type} –º–æ–¥–µ–ª—å: {model_name}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω–∞ –ª–∏ —É–∂–µ –º–æ–¥–µ–ª—å
            model_path = self.cache_dir / model_name.replace("/", "--")
            if model_path.exists() and any(model_path.iterdir()):
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É–∂–µ —Å–∫–∞—á–∞–Ω–∞")
                return True
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            def download_model():
                return snapshot_download(
                    repo_id=model_name,
                    cache_dir=str(self.cache_dir),
                    resume_download=True,
                    local_files_only=False
                )
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            result = await asyncio.to_thread(download_model)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞ –≤ {result}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏
            await self._test_model_loading(model_name, model_type)
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_info['name']}: {e}")
            return False

    async def _test_model_loading(self, model_name: str, model_type: str):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ {model_name}")
            
            def test_load():
                if model_type == "llm":
                    from transformers import AutoTokenizer, AutoModelForCausalLM
                    tokenizer = AutoTokenizer.from_pretrained(
                        model_name, 
                        cache_dir=str(self.cache_dir)
                    )
                    model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        cache_dir=str(self.cache_dir),
                        torch_dtype=torch.float32,
                        device_map="cpu"
                    )
                    return True
                    
                elif model_type == "vision":
                    from transformers import BlipProcessor, BlipForConditionalGeneration
                    processor = BlipProcessor.from_pretrained(
                        model_name,
                        cache_dir=str(self.cache_dir)
                    )
                    model = BlipForConditionalGeneration.from_pretrained(
                        model_name,
                        cache_dir=str(self.cache_dir),
                        torch_dtype=torch.float32
                    )
                    return True
                    
                elif model_type == "embeddings":
                    from sentence_transformers import SentenceTransformer
                    model = SentenceTransformer(
                        model_name,
                        cache_folder=str(self.cache_dir)
                    )
                    return True
                    
                return True
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É
            success = await asyncio.to_thread(test_load)
            
            if success:
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}: {e}")

    async def install_ollama_models(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ Ollama"""
        try:
            logger.info("ü¶ô –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama –º–æ–¥–µ–ª–∏")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ Ollama
            ollama_check = await asyncio.create_subprocess_exec(
                "which", "ollama",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await ollama_check.wait()
            
            if ollama_check.returncode != 0:
                logger.info("üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama")
                await self._install_ollama()
            
            # –ú–æ–¥–µ–ª–∏ –¥–ª—è Ollama
            ollama_models = [
                "llama3.2:3b",
                "phi3:3.8b", 
                "qwen2.5:7b",
                "nomic-embed-text"
            ]
            
            for model in ollama_models:
                try:
                    logger.info(f"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama –º–æ–¥–µ–ª—å: {model}")
                    
                    process = await asyncio.create_subprocess_exec(
                        "ollama", "pull", model,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    
                    stdout, stderr = await process.communicate()
                    
                    if process.returncode == 0:
                        logger.info(f"‚úÖ Ollama –º–æ–¥–µ–ª—å {model} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {model}: {stderr.decode()}")
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama –º–æ–¥–µ–ª–∏ {model}: {e}")
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã —Å Ollama: {e}")

    async def _install_ollama(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama"""
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama
            install_cmd = "curl -fsSL https://ollama.ai/install.sh | sh"
            
            process = await asyncio.create_subprocess_shell(
                install_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.info("‚úÖ Ollama —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º Ollama —Å–µ—Ä–≤–µ—Ä –≤ —Ñ–æ–Ω–µ
                await asyncio.create_subprocess_exec(
                    "ollama", "serve"
                )
                
                # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
                await asyncio.sleep(10)
                
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {stderr.decode()}")
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama")
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {e}")
            raise

    async def check_disk_space(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ"""
        try:
            import shutil
            total, used, free = shutil.disk_usage("/")
            
            free_gb = free // (1024**3)
            logger.info(f"üíæ –°–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ: {free_gb}GB")
            
            if free_gb < 100:  # –ú–∏–Ω–∏–º—É–º 100GB –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
                logger.warning(f"‚ö†Ô∏è –ú–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞: {free_gb}GB")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∏—Å–∫–∞: {e}")
            return False

    async def generate_download_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"""
        try:
            logger.info("üìä –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –æ –º–æ–¥–µ–ª—è—Ö")
            
            report = {
                "download_date": datetime.now().isoformat(),
                "models_directory": str(self.models_dir),
                "total_models": 0,
                "models_by_type": {},
                "total_size_gb": 0,
                "models_list": []
            }
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            for model_dir in self.cache_dir.iterdir():
                if model_dir.is_dir():
                    model_name = model_dir.name.replace("--", "/")
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
                    model_type = "unknown"
                    if "llama" in model_name.lower() or "phi" in model_name.lower() or "qwen" in model_name.lower():
                        model_type = "llm"
                    elif "blip" in model_name.lower() or "clip" in model_name.lower():
                        model_type = "vision"
                    elif "stable-diffusion" in model_name.lower():
                        model_type = "image_gen"
                    elif "whisper" in model_name.lower():
                        model_type = "stt"
                    elif "sentence-transformers" in model_name.lower():
                        model_type = "embeddings"
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                    size_bytes = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
                    size_gb = size_bytes / (1024**3)
                    
                    model_info = {
                        "name": model_name,
                        "type": model_type,
                        "size_gb": round(size_gb, 2),
                        "path": str(model_dir)
                    }
                    
                    report["models_list"].append(model_info)
                    report["total_models"] += 1
                    report["total_size_gb"] += size_gb
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
                    if model_type not in report["models_by_type"]:
                        report["models_by_type"][model_type] = []
                    report["models_by_type"][model_type].append(model_name)
            
            report["total_size_gb"] = round(report["total_size_gb"], 2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            report_file = self.models_dir / "download_report.json"
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
            logger.info(f"üìà –í—Å–µ–≥–æ —Å–∫–∞—á–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {report['total_models']}")
            logger.info(f"üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {report['total_size_gb']}GB")
            
            return report
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
            return None


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π AGI Layer v3.9")
    
    downloader = ModelDownloader()
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ
        if not await downloader.check_disk_space():
            logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ")
            sys.exit(1)
        
        # –°–∫–∞—á–∏–≤–∞–µ–º HuggingFace –º–æ–¥–µ–ª–∏
        logger.info("üì¶ –°–∫–∞—á–∏–≤–∞–µ–º HuggingFace –º–æ–¥–µ–ª–∏...")
        await downloader.download_all_models()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama –º–æ–¥–µ–ª–∏
        logger.info("ü¶ô –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama –º–æ–¥–µ–ª–∏...")
        await downloader.install_ollama_models()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = await downloader.generate_download_report()
        
        if report:
            logger.info("üéâ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {report['total_models']} –º–æ–¥–µ–ª–µ–π, {report['total_size_gb']}GB")
        else:
            logger.warning("‚ö†Ô∏è –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())







