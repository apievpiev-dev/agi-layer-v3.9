#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ CPU-only –º–æ–¥–µ–ª–µ–π –¥–ª—è AGI Layer v3.9
"""

import os
import sys
import argparse
import asyncio
import logging
from pathlib import Path
from typing import Dict, List
import requests
from tqdm import tqdm

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from config.models import MODELS, get_model_config, get_total_size_mb


class ModelDownloader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, models_path: str = "/app/models"):
        self.models_path = Path(models_path)
        self.models_path.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)
        
    def download_model(self, model_name: str, force: bool = False) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            model_config = get_model_config(model_name)
            model_dir = self.models_path / model_name
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å
            if model_dir.exists() and not force:
                self.logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                return True
            
            self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
            model_dir.mkdir(exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ HuggingFace
            if model_name == "stable_diffusion_1_5":
                self._download_stable_diffusion(model_dir)
            elif model_name == "phi_2":
                self._download_phi2(model_dir)
            elif model_name == "blip2":
                self._download_blip2(model_dir)
            elif model_name == "easyocr":
                self._download_easyocr(model_dir)
            elif model_name == "sentence_transformers":
                self._download_sentence_transformers(model_dir)
            else:
                self.logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
                return False
            
            self.logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    def _download_stable_diffusion(self, model_dir: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Stable Diffusion 1.5"""
        from diffusers import StableDiffusionPipeline
        
        self.logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ Stable Diffusion 1.5...")
        pipeline = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype="float32",
            use_safetensors=True
        )
        
        pipeline.save_pretrained(model_dir)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            "model_name": "stable_diffusion_1_5",
            "type": "image_gen",
            "framework": "diffusers",
            "loaded": True
        }
        
        import json
        with open(model_dir / "config.json", "w") as f:
            json.dump(config, f, indent=2)
    
    def _download_phi2(self, model_dir: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Phi-2"""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        self.logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ Phi-2...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        tokenizer = AutoTokenizer.from_pretrained(
            "microsoft/phi-2",
            trust_remote_code=True
        )
        tokenizer.save_pretrained(model_dir)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model = AutoModelForCausalLM.from_pretrained(
            "microsoft/phi-2",
            torch_dtype="float32",
            trust_remote_code=True,
            device_map="cpu"
        )
        model.save_pretrained(model_dir)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            "model_name": "phi_2",
            "type": "llm",
            "framework": "transformers",
            "loaded": True
        }
        
        import json
        with open(model_dir / "config.json", "w") as f:
            json.dump(config, f, indent=2)
    
    def _download_blip2(self, model_dir: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ BLIP2"""
        from transformers import Blip2Processor, Blip2ForConditionalGeneration
        
        self.logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ BLIP2...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = Blip2Processor.from_pretrained(
            "Salesforce/blip2-opt-2.7b"
        )
        processor.save_pretrained(model_dir)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            torch_dtype="float32",
            device_map="cpu"
        )
        model.save_pretrained(model_dir)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            "model_name": "blip2",
            "type": "vision",
            "framework": "transformers",
            "loaded": True
        }
        
        import json
        with open(model_dir / "config.json", "w") as f:
            json.dump(config, f, indent=2)
    
    def _download_easyocr(self, model_dir: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ EasyOCR"""
        import easyocr
        
        self.logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ EasyOCR...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EasyOCR –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
        reader = easyocr.Reader(
            ['en', 'ru'],
            gpu=False,
            model_storage_directory=str(model_dir)
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            "model_name": "easyocr",
            "type": "ocr",
            "framework": "easyocr",
            "languages": ["en", "ru"],
            "loaded": True
        }
        
        import json
        with open(model_dir / "config.json", "w") as f:
            json.dump(config, f, indent=2)
    
    def _download_sentence_transformers(self, model_dir: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ SentenceTransformers"""
        from sentence_transformers import SentenceTransformer
        
        self.logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ SentenceTransformers...")
        
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        model.save(model_dir)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            "model_name": "sentence_transformers",
            "type": "embedding",
            "framework": "sentence_transformers",
            "model_type": "all-MiniLM-L6-v2",
            "embedding_dimension": 384,
            "loaded": True
        }
        
        import json
        with open(model_dir / "config.json", "w") as f:
            json.dump(config, f, indent=2)
    
    def download_all_models(self, force: bool = False) -> Dict[str, bool]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        results = {}
        
        total_size = get_total_size_mb()
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π (–æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size} MB)")
        
        for model_name in MODELS.keys():
            print(f"\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ {model_name}...")
            results[model_name] = self.download_model(model_name, force)
        
        return results
    
    def check_models(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–µ–π"""
        results = {}
        
        for model_name in MODELS.keys():
            model_dir = self.models_path / model_name
            config_file = model_dir / "config.json"
            
            exists = model_dir.exists() and config_file.exists()
            results[model_name] = exists
            
            status = "‚úÖ" if exists else "‚ùå"
            print(f"{status} {model_name}: {'–ì–æ—Ç–æ–≤' if exists else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
        
        return results
    
    def get_disk_usage(self) -> Dict[str, int]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –¥–∏—Å–∫–∞"""
        usage = {}
        
        for model_name in MODELS.keys():
            model_dir = self.models_path / model_name
            if model_dir.exists():
                total_size = sum(
                    f.stat().st_size 
                    for f in model_dir.rglob('*') 
                    if f.is_file()
                )
                usage[model_name] = total_size // (1024 * 1024)  # MB
            else:
                usage[model_name] = 0
        
        return usage


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π AGI Layer v3.9")
    parser.add_argument("--models-path", default="/app/models", help="–ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º")
    parser.add_argument("--model", help="–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å")
    parser.add_argument("--all", action="store_true", help="–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏")
    parser.add_argument("--check", action="store_true", help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–µ–π")
    parser.add_argument("--check-only", action="store_true", help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏")
    parser.add_argument("--force", action="store_true", help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞")
    parser.add_argument("--usage", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞")
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    downloader = ModelDownloader(args.models_path)
    
    if args.check or args.check_only:
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π:")
        results = downloader.check_models()
        
        if args.check_only:
            return
        
        missing_models = [name for name, exists in results.items() if not exists]
        if missing_models and args.all:
            print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π: {missing_models}")
            for model_name in missing_models:
                downloader.download_model(model_name, args.force)
    
    elif args.usage:
        print("üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞:")
        usage = downloader.get_disk_usage()
        total = 0
        for model_name, size in usage.items():
            print(f"  {model_name}: {size} MB")
            total += size
        print(f"  –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total} MB")
    
    elif args.model:
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {args.model}")
        success = downloader.download_model(args.model, args.force)
        if success:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {args.model} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {args.model}")
            sys.exit(1)
    
    elif args.all:
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π")
        results = downloader.download_all_models(args.force)
        
        failed_models = [name for name, success in results.items() if not success]
        if failed_models:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {failed_models}")
            sys.exit(1)
        else:
            print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

