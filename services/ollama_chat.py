"""
Ollama Chat Service - –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —á–∞—Ç–∞ —Å Ollama
"""

import asyncio
import aiohttp
import logging
import json
from typing import Dict, Any, List, Optional
import streamlit as st
from datetime import datetime


class OllamaChatService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Ollama"""
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        self.logger = logging.getLogger(__name__)
        self.http_session = None
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTTP —Å–µ—Å—Å–∏–∏"""
        self.http_session = aiohttp.ClientSession()
        
    async def get_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            async with self.http_session.get(f"{self.ollama_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('models', [])
                else:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {response.status}")
                    return []
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return []
    
    async def generate_response(self, model: str, prompt: str, context: List[str] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏"""
        try:
            data = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "context": context or []
            }
            
            async with self.http_session.post(
                f"{self.ollama_url}/api/generate", 
                json=data
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get('response', '')
                else:
                    error_text = await response.text()
                    self.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {response.status} - {error_text}")
                    return f"–û—à–∏–±–∫–∞: {response.status}"
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}"
    
    async def stream_response(self, model: str, prompt: str, context: List[str] = None):
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        try:
            data = {
                "model": model,
                "prompt": prompt,
                "stream": True,
                "context": context or []
            }
            
            async with self.http_session.post(
                f"{self.ollama_url}/api/generate", 
                json=data
            ) as response:
                if response.status == 200:
                    async for line in response.content:
                        if line:
                            try:
                                chunk = json.loads(line.decode('utf-8'))
                                if 'response' in chunk:
                                    yield chunk['response']
                                if chunk.get('done', False):
                                    break
                            except json.JSONDecodeError:
                                continue
                else:
                    error_text = await response.text()
                    yield f"–û—à–∏–±–∫–∞: {response.status} - {error_text}"
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            yield f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}"
    
    async def check_ollama_status(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            async with self.http_session.get(f"{self.ollama_url}/api/tags") as response:
                return response.status == 200
        except:
            return False


class OllamaChatUI:
    """–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —á–∞—Ç–∞ —Å Ollama"""
    
    def __init__(self, ollama_service: OllamaChatService):
        self.ollama_service = ollama_service
        
    async def render_chat_page(self):
        """–û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–∞—Ç–∞"""
        st.header("ü§ñ –ß–∞—Ç —Å Ollama")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama
        ollama_status = await self.ollama_service.check_ollama_status()
        
        if not ollama_status:
            st.error("‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –Ω–∞ localhost:11434")
            st.info("–î–ª—è –∑–∞–ø—É—Å–∫–∞ Ollama –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É: `ollama serve`")
            return
        
        st.success("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        models = await self.ollama_service.get_models()
        
        if not models:
            st.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π: `ollama pull llama2`")
            return
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞—Ç–∞
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            model_names = [model['name'] for model in models]
            selected_model = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
                model_names,
                help="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞"
            )
        
        with col2:
            # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
            if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç", use_container_width=True):
                st.session_state.ollama_chat_history = []
                st.session_state.ollama_context = []
                st.rerun()
        
        # –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
        if 'ollama_chat_history' not in st.session_state:
            st.session_state.ollama_chat_history = []
        
        if 'ollama_context' not in st.session_state:
            st.session_state.ollama_context = []
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
        chat_container = st.container()
        
        with chat_container:
            for i, message in enumerate(st.session_state.ollama_chat_history):
                if message['role'] == 'user':
                    with st.chat_message("user"):
                        st.write(message['content'])
                else:
                    with st.chat_message("assistant"):
                        st.write(message['content'])
        
        # –ü–æ–ª–µ –≤–≤–æ–¥–∞
        user_input = st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        
        if user_input:
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
            st.session_state.ollama_chat_history.append({
                'role': 'user',
                'content': user_input,
                'timestamp': datetime.now()
            })
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            with chat_container:
                with st.chat_message("user"):
                    st.write(user_input)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            with st.chat_message("assistant"):
                response_placeholder = st.empty()
                full_response = ""
                
                # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                context_messages = []
                for msg in st.session_state.ollama_chat_history[-10:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                    if msg['role'] == 'user':
                        context_messages.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {msg['content']}")
                    else:
                        context_messages.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {msg['content']}")
                
                context_prompt = "\n".join(context_messages)
                context_prompt += f"\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
                
                # –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                try:
                    async for chunk in self.ollama_service.stream_response(
                        selected_model, 
                        context_prompt, 
                        st.session_state.ollama_context
                    ):
                        full_response += chunk
                        response_placeholder.write(full_response + "‚ñå")
                    
                    # –£–±–∏—Ä–∞–µ–º –∫—É—Ä—Å–æ—Ä
                    response_placeholder.write(full_response)
                    
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
                    st.session_state.ollama_chat_history.append({
                        'role': 'assistant',
                        'content': full_response,
                        'timestamp': datetime.now()
                    })
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}"
                    response_placeholder.write(error_msg)
                    st.session_state.ollama_chat_history.append({
                        'role': 'assistant',
                        'content': error_msg,
                        'timestamp': datetime.now()
                    })
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        if models:
            selected_model_info = next((m for m in models if m['name'] == selected_model), None)
            if selected_model_info:
                with st.expander("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"):
                    st.write(f"**–ù–∞–∑–≤–∞–Ω–∏–µ:** {selected_model_info['name']}")
                    st.write(f"**–†–∞–∑–º–µ—Ä:** {selected_model_info.get('size', 'N/A')} –±–∞–π—Ç")
                    st.write(f"**–î–∞—Ç–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏:** {selected_model_info.get('modified_at', 'N/A')}")
                    st.write(f"**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** {len(st.session_state.ollama_context)} —Å–æ–æ–±—â–µ–Ω–∏–π")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —á–∞—Ç–∞"""
    ollama_service = OllamaChatService()
    await ollama_service.initialize()
    
    chat_ui = OllamaChatUI(ollama_service)
    await chat_ui.render_chat_page()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())