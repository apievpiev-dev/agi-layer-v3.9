"""
Ollama Chat Web Interface - –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —á–∞—Ç–∞ —Å Ollama
"""

import asyncio
import logging
import streamlit as st
import aiohttp
import json
from typing import Dict, Any, List, AsyncGenerator
from datetime import datetime
import os


class OllamaChatUI:
    """–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —á–∞—Ç–∞ —Å Ollama"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.http_session = None
        self.ollama_host = config.get('ollama_host', 'localhost')
        self.ollama_port = config.get('ollama_port', 11434)
        self.ollama_base_url = f"http://{self.ollama_host}:{self.ollama_port}"
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Web UI"""
        self.http_session = aiohttp.ClientSession()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama
        try:
            async with self.http_session.get(f"{self.ollama_base_url}/api/tags") as response:
                if response.status == 200:
                    st.success("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                else:
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama")
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
    
    async def run(self):
        """–ó–∞–ø—É—Å–∫ Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        st.set_page_config(
            page_title="Ollama Chat - AGI Layer",
            page_icon="ü§ñ",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        st.title("ü§ñ Ollama Chat - –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å")
        st.markdown("---")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        
        if 'current_model' not in st.session_state:
            st.session_state.current_model = 'llama2'
        
        # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
        await self._render_sidebar()
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —á–∞—Ç
        await self._render_chat()
    
    async def _render_sidebar(self):
        """–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏"""
        with st.sidebar:
            st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞—Ç–∞")
            
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            st.subheader("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏")
            try:
                models = await self._get_available_models()
                if models:
                    selected_model = st.selectbox(
                        "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
                        models,
                        index=models.index(st.session_state.current_model) if st.session_state.current_model in models else 0
                    )
                    if selected_model != st.session_state.current_model:
                        st.session_state.current_model = selected_model
                        st.rerun()
                else:
                    st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
                    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π: `ollama pull llama2`")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            
            st.markdown("---")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            st.subheader("üéõÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
            
            temperature = st.slider(
                "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å):",
                min_value=0.0,
                max_value=2.0,
                value=0.7,
                step=0.1,
                help="–í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º–∏"
            )
            
            max_tokens = st.slider(
                "–ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤:",
                min_value=100,
                max_value=4000,
                value=1000,
                step=100,
                help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞"
            )
            
            top_p = st.slider(
                "Top-p (nucleus sampling):",
                min_value=0.0,
                max_value=1.0,
                value=0.9,
                step=0.05,
                help="–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ—Ç–≤–µ—Ç–æ–≤"
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Å–µ—Å—Å–∏–∏
            st.session_state.generation_params = {
                'temperature': temperature,
                'max_tokens': max_tokens,
                'top_p': top_p
            }
            
            st.markdown("---")
            
            # –î–µ–π—Å—Ç–≤–∏—è
            st.subheader("üõ†Ô∏è –î–µ–π—Å—Ç–≤–∏—è")
            
            if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç", use_container_width=True):
                st.session_state.messages = []
                st.rerun()
            
            if st.button("üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", use_container_width=True):
                model_name = st.text_input("–ò–º—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: llama2):")
                if model_name:
                    with st.spinner(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}..."):
                        success = await self._pull_model(model_name)
                        if success:
                            st.success(f"–ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                        else:
                            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
            
            if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π", use_container_width=True):
                st.rerun()
            
            st.markdown("---")
            
            # –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã
            await self._render_system_status()
    
    async def _render_system_status(self):
        """–û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã"""
        st.subheader("üìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama
            async with self.http_session.get(f"{self.ollama_base_url}/api/tags") as response:
                if response.status == 200:
                    st.success("üü¢ Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç")
                else:
                    st.error("üî¥ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        except Exception as e:
            st.error(f"üî¥ –û—à–∏–±–∫–∞: {e}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        st.info(f"**–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å:** {st.session_state.current_model}")
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
        st.info(f"**–°–æ–æ–±—â–µ–Ω–∏–π –≤ —á–∞—Ç–µ:** {len(st.session_state.messages)}")
    
    async def _render_chat(self):
        """–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —á–∞—Ç–∞"""
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # –ü–æ–ª–µ –≤–≤–æ–¥–∞
        if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."):
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                try:
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    params = st.session_state.get('generation_params', {
                        'temperature': 0.7,
                        'max_tokens': 1000,
                        'top_p': 0.9
                    })
                    
                    # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama
                    async for chunk in self._generate_response(prompt, params):
                        full_response += chunk
                        message_placeholder.markdown(full_response + "‚ñå")
                    
                    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    message_placeholder.markdown(full_response)
                    
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}"
                    message_placeholder.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
    
    async def _get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            async with self.http_session.get(f"{self.ollama_base_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    return models
                else:
                    return []
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    async def _generate_response(self, prompt: str, params: Dict[str, Any]) -> AsyncGenerator[str, None]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama API"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            messages = []
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            system_prompt = "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –∑–∞–¥–∞–Ω –Ω–∞ —Ä—É—Å—Å–∫–æ–º."
            messages.append({"role": "system", "content": system_prompt})
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
            for msg in st.session_state.messages[-10:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                messages.append({"role": msg["role"], "content": msg["content"]})
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            messages.append({"role": "user", "content": prompt})
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            request_data = {
                "model": st.session_state.current_model,
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": params.get('temperature', 0.7),
                    "num_predict": params.get('max_tokens', 1000),
                    "top_p": params.get('top_p', 0.9)
                }
            }
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            async with self.http_session.post(
                f"{self.ollama_base_url}/api/chat",
                json=request_data
            ) as response:
                if response.status == 200:
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if line:
                            try:
                                data = json.loads(line)
                                if 'message' in data and 'content' in data['message']:
                                    yield data['message']['content']
                            except json.JSONDecodeError:
                                continue
                else:
                    error_text = await response.text()
                    yield f"–û—à–∏–±–∫–∞ API: {response.status} - {error_text}"
                    
        except Exception as e:
            yield f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"
    
    async def _pull_model(self, model_name: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            async with self.http_session.post(
                f"{self.ollama_base_url}/api/pull",
                json={"name": model_name}
            ) as response:
                return response.status == 200
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.http_session:
            await self.http_session.close()


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ Ollama Chat UI"""
    config = {
        'ollama_host': os.getenv('OLLAMA_HOST', 'localhost'),
        'ollama_port': int(os.getenv('OLLAMA_PORT', 11434))
    }
    
    chat_ui = OllamaChatUI(config)
    
    try:
        await chat_ui.initialize()
        await chat_ui.run()
    finally:
        await chat_ui.cleanup()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())