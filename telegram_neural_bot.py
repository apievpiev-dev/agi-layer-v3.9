#!/usr/bin/env python3
"""
–†–ï–ê–õ–¨–ù–´–ô Telegram –±–æ—Ç —Å –ù–ê–°–¢–û–Ø–©–ò–ú–ò –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏ AGI Layer v3.9
–ü–æ–¥–∫–ª—é—á–µ–Ω–∞ –º–æ–¥–µ–ª—å Phi-2 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ + –ø–∞–º—è—Ç—å
"""

import asyncio
import logging
import json
import os
import sys
from datetime import datetime
from typing import Dict, Any, List, Optional
import urllib.request
import urllib.parse
import urllib.error

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, '/workspace')

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    NEURAL_AVAILABLE = True
    print("üß† PyTorch –∏ Transformers –¥–æ—Å—Ç—É–ø–Ω—ã!")
except ImportError:
    NEURAL_AVAILABLE = False
    print("‚ö†Ô∏è PyTorch/Transformers –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã - —Ä–∞–±–æ—Ç–∞–µ–º –≤ –¥–µ–º–æ —Ä–µ–∂–∏–º–µ")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class RealNeuralAI:
    """–ù–ê–°–¢–û–Ø–©–ò–ô –ò–ò —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é Phi-2"""
    
    def __init__(self):
        self.device = "cpu"
        self.model = None
        self.tokenizer = None
        self.model_loaded = False
        self.chat_memory: Dict[int, List[Dict]] = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        if NEURAL_AVAILABLE:
            asyncio.create_task(self._load_model())
    
    async def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Phi-2"""
        try:
            logger.info("üß† –ó–∞–≥—Ä—É–∂–∞—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å Phi-2...")
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ –∏–ª–∏ HuggingFace
            model_path = "/workspace/models/phi_2"
            if not os.path.exists(model_path):
                model_path = "microsoft/phi-2"
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                pad_token="<|endoftext|>"
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                trust_remote_code=True,
                device_map="cpu",
                low_cpu_mem_usage=True
            )
            
            self.model_loaded = True
            logger.info("‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç—å Phi-2 –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {e}")
            self.model_loaded = False
    
    def get_memory(self, chat_id: int) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞–º—è—Ç—å —á–∞—Ç–∞"""
        if chat_id not in self.chat_memory:
            self.chat_memory[chat_id] = []
        return self.chat_memory[chat_id]
    
    def add_to_memory(self, chat_id: int, role: str, content: str):
        """–î–æ–±–∞–≤–∏—Ç—å –≤ –ø–∞–º—è—Ç—å"""
        memory = self.get_memory(chat_id)
        memory.append({
            'role': role,
            'content': content,
            'timestamp': datetime.now().isoformat()
        })
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å
        if len(memory) > 20:
            self.chat_memory[chat_id] = memory[-20:]
    
    async def generate_response(self, chat_id: int, message: str, user_name: str = "User") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
        self.add_to_memory(chat_id, 'user', message)
        
        if not self.model_loaded or not NEURAL_AVAILABLE:
            return await self._fallback_response(chat_id, message, user_name)
        
        try:
            # –°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏
            context = self._build_context(chat_id, message)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Phi-2
            response = await self._neural_generate(context)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –ø–∞–º—è—Ç—å
            self.add_to_memory(chat_id, 'assistant', response)
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return await self._fallback_response(chat_id, message, user_name)
    
    def _build_context(self, chat_id: int, current_message: str) -> str:
        """–°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        memory = self.get_memory(chat_id)
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = """–¢—ã —É–º–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç AGI Layer v3.9. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É. 
–ë—É–¥—å –ø–æ–ª–µ–∑–Ω—ã–º, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º. –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è."""
        
        # –°—Ç—Ä–æ–∏–º –¥–∏–∞–ª–æ–≥
        context_parts = [system_prompt]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        recent_memory = memory[-10:] if memory else []
        for msg in recent_memory:
            if msg['role'] == 'user':
                context_parts.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {msg['content']}")
            else:
                context_parts.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {msg['content']}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        context_parts.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {current_message}")
        context_parts.append("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")
        
        return "\n".join(context_parts)
    
    async def _neural_generate(self, context: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å"""
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer.encode(context, return_tensors="pt", max_length=1024, truncation=True)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 150,  # –î–æ–±–∞–≤–ª—è–µ–º 150 —Ç–æ–∫–µ–Ω–æ–≤ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3,
                    repetition_penalty=1.1
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            if "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:" in generated_text:
                response = generated_text.split("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")[-1].strip()
            else:
                response = generated_text[len(context):].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = self._clean_response(response)
            
            return response if response else "–ü–æ–Ω—è–ª –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –Ω–æ –∑–∞—Ç—Ä—É–¥–Ω—è—é—Å—å —Å –æ—Ç–≤–µ—Ç–æ–º. –ú–æ–∂–µ—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å?"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞"""
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—ã "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:" –∏ "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
        response = response.split("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:")[0]
        response = response.split("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")[0]
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        response = response.strip()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(response) > 500:
            sentences = response.split('.')
            response = '. '.join(sentences[:3]) + '.'
        
        return response
    
    async def _fallback_response(self, chat_id: int, message: str, user_name: str) -> str:
        """Fallback –æ—Ç–≤–µ—Ç—ã –∫–æ–≥–¥–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å']):
            return f"–ü—Ä–∏–≤–µ—Ç, {user_name}! –Ø AGI Layer v3.9. –ù–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ–∫–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –Ω–æ —è –≥–æ—Ç–æ–≤ –æ–±—â–∞—Ç—å—Å—è!"
        
        if '–¥–µ–ª–∞' in message_lower:
            return f"–û—Ç–ª–∏—á–Ω–æ! –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≥–æ—Ç–æ–≤–∏—Ç—Å—è –∫ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ. –ê –∫–∞–∫ —É –≤–∞—Å –¥–µ–ª–∞?"
        
        if '–ø–∞–º—è—Ç—å' in message_lower or '–ø–æ–º–Ω–∏—à—å' in message_lower:
            memory_count = len(self.get_memory(chat_id))
            return f"–î–∞, –ø–æ–º–Ω—é –Ω–∞—à –¥–∏–∞–ª–æ–≥! –£ –Ω–∞—Å —É–∂–µ {memory_count//2} –æ–±–º–µ–Ω–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏. –ù–µ–π—Ä–æ—Å–µ—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç."
        
        return f"–ü–æ–Ω—è–ª –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ '{message}'. –ù–µ–π—Ä–æ—Å–µ—Ç—å Phi-2 –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å... –ß—Ç–æ –∏–º–µ–Ω–Ω–æ —Ö–æ—Ç–µ–ª–∏ —É–∑–Ω–∞—Ç—å?"


class NeuralTelegramBot:
    """Telegram –±–æ—Ç —Å –Ω–∞—Å—Ç–æ—è—â–∏–º–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏"""
    
    def __init__(self):
        self.token = "8325306099:AAG6hk3tG2-XmiJPgegzYFzQcY6WJaEbRxw"
        self.allowed_chat_id = "458589236"
        self.api_url = f"https://api.telegram.org/bot{self.token}"
        self.last_update_id = 0
        
        # –ù–ê–°–¢–û–Ø–©–ò–ô –ò–ò —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é
        self.ai = RealNeuralAI()
    
    def send_message_sync(self, chat_id: str, text: str) -> bool:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        try:
            url = f"{self.api_url}/sendMessage"
            data = {
                "chat_id": chat_id,
                "text": text[:4096],
                "parse_mode": "HTML"
            }
            
            data_encoded = urllib.parse.urlencode(data).encode('utf-8')
            req = urllib.request.Request(url, data=data_encoded, method='POST')
            req.add_header('Content-Type', 'application/x-www-form-urlencoded')
            
            with urllib.request.urlopen(req, timeout=10) as response:
                if response.status == 200:
                    result = json.loads(response.read().decode('utf-8'))
                    if result.get('ok'):
                        logger.info(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {text[:100]}...")
                        return True
                
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.status}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
            return False
    
    def get_updates_sync(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π"""
        try:
            url = f"{self.api_url}/getUpdates"
            params = {
                "offset": self.last_update_id + 1,
                "timeout": 10
            }
            
            query_string = urllib.parse.urlencode(params)
            full_url = f"{url}?{query_string}"
            
            with urllib.request.urlopen(full_url, timeout=15) as response:
                if response.status == 200:
                    data = json.loads(response.read().decode('utf-8'))
                    if data.get('ok'):
                        return data.get('result', [])
                        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {e}")
        
        return []
    
    async def process_update(self, update: Dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
        try:
            if 'message' not in update:
                return
            
            message = update['message']
            chat_id = str(message['chat']['id'])
            text = message.get('text', '')
            user_name = message['from'].get('first_name', '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å')
            
            logger.info(f"üì® –û—Ç {user_name}: {text}")
            
            # –ö–æ–º–∞–Ω–¥—ã
            if text == '/start':
                neural_status = "üß† –ê–∫—Ç–∏–≤–Ω–∞" if self.ai.model_loaded else "‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è"
                response = f"""üß† <b>AGI Layer v3.9 - –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ô –ò–ò</b>

–ü—Ä–∏–≤–µ—Ç, {user_name}! –Ø –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ –ù–ê–°–¢–û–Ø–©–ï–ô –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ Phi-2!

ü§ñ <b>–°—Ç–∞—Ç—É—Å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:</b> {neural_status}
üíæ <b>–ü–∞–º—è—Ç—å:</b> –ü–æ–º–Ω—é –≤–µ—Å—å –Ω–∞—à –¥–∏–∞–ª–æ–≥
üéØ <b>–†–µ–∂–∏–º:</b> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Phi-2

<b>–ü–∏—à–∏—Ç–µ –ª—é–±—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è - –æ—Ç–≤–µ—á–∞—é —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å!</b> üöÄ

{f'‚ö° <b>–ù–µ–π—Ä–æ—Å–µ—Ç—å –≥–æ—Ç–æ–≤–∞ –∫ –¥–∏–∞–ª–æ–≥—É!</b>' if self.ai.model_loaded else '‚è≥ <b>–ù–µ–π—Ä–æ—Å–µ—Ç—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –Ω–æ —É–∂–µ –æ—Ç–≤–µ—á–∞—é!</b>'}"""
                
            elif text == '/status':
                memory_count = len(self.ai.get_memory(int(chat_id)))
                model_status = "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞" if self.ai.model_loaded else "‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è"
                
                response = f"""üìä <b>–°—Ç–∞—Ç—É—Å –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ì–û –ò–ò</b>

üß† <b>Phi-2 –º–æ–¥–µ–ª—å:</b> {model_status}
üíæ <b>–ü–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–∞:</b> {memory_count} —Å–æ–æ–±—â–µ–Ω–∏–π
üéØ <b>–†–µ–∂–∏–º:</b> {'–ù–µ–π—Ä–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è' if self.ai.model_loaded else '–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞'}
‚ö° <b>PyTorch:</b> {'–î–æ—Å—Ç—É–ø–µ–Ω' if NEURAL_AVAILABLE else '–ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}

<b>–ö–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é Phi-2!</b> ü§ñ"""
                
            elif text == '/memory':
                memory = self.ai.get_memory(int(chat_id))
                if memory:
                    recent = memory[-5:]
                    memory_text = "\n".join([f"{'üë§' if m['role']=='user' else 'ü§ñ'} {m['content'][:50]}..." for m in recent])
                    response = f"üíæ <b>–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:</b>\n\n{memory_text}"
                else:
                    response = "üíæ <b>–ü–∞–º—è—Ç—å –ø—É—Å—Ç–∞</b> - –Ω–∞—á–Ω–∏—Ç–µ –¥–∏–∞–ª–æ–≥!"
                
            else:
                # –ù–ï–ô–†–û–°–ï–¢–ï–í–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                response = await self.ai.generate_response(int(chat_id), text, user_name)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
            self.send_message_sync(chat_id, response)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            self.send_message_sync(chat_id, f"‚ùå –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {str(e)}")
    
    async def run(self):
        """–ó–∞–ø—É—Å–∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –±–æ—Ç–∞"""
        logger.info("üß† –ó–∞–ø—É—Å–∫ –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ì–û Telegram –±–æ—Ç–∞ AGI Layer v3.9")
        
        # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
        if self.allowed_chat_id:
            neural_status = "üß† –ê–∫—Ç–∏–≤–Ω–∞" if self.ai.model_loaded else "‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è"
            success = self.send_message_sync(
                self.allowed_chat_id,
                f"üß† <b>–ù–ï–ô–†–û–°–ï–¢–ï–í–û–ô AGI Layer v3.9 –∑–∞–ø—É—â–µ–Ω!</b>\n\nü§ñ Phi-2: {neural_status}\nüíæ –ü–∞–º—è—Ç—å: –ê–∫—Ç–∏–≤–Ω–∞\n\n–¢–µ–ø–µ—Ä—å –∫–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ù–ê–°–¢–û–Ø–©–ï–ô –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é! –ü–∏—à–∏—Ç–µ /start"
            )
            if success:
                logger.info("‚úÖ –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ —Ü–∏–∫–ª–∞...")
        while True:
            try:
                updates = self.get_updates_sync()
                
                for update in updates:
                    self.last_update_id = update['update_id']
                    await self.process_update(update)
                
                await asyncio.sleep(1)
                
            except KeyboardInterrupt:
                logger.info("üõë –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –±–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                break
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ: {e}")
                await asyncio.sleep(5)


async def main():
    """–ó–∞–ø—É—Å–∫ –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ì–û –±–æ—Ç–∞"""
    print("üß† AGI Layer v3.9 - –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ô Telegram Bot")
    print("=" * 60)
    print("ü§ñ –ù–ê–°–¢–û–Ø–©–ê–Ø –Ω–µ–π—Ä–æ—Å–µ—Ç—å Phi-2")
    print("üíæ –ü–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–∞")
    print("‚ö° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ PyTorch")
    print("üéØ –ë–ï–ó –∑–∞–≥–æ—Ç–æ–≤–æ–∫ - —Ç–æ–ª—å–∫–æ –ò–ò!")
    print()
    
    if not NEURAL_AVAILABLE:
        print("‚ö†Ô∏è PyTorch/Transformers –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
        print("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch transformers")
        print("üîß –†–∞–±–æ—Ç–∞–µ–º –≤ –¥–µ–º–æ —Ä–µ–∂–∏–º–µ —Å —É–º–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏")
        print()
    
    bot = NeuralTelegramBot()
    await bot.run()


if __name__ == "__main__":
    asyncio.run(main())