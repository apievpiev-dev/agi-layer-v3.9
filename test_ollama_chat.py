#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –≤–µ–±-—á–∞—Ç–∞ —Å Ollama
"""

import asyncio
import aiohttp
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.ollama_chat import OllamaChatService


async def test_ollama_connection():
    """–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama...")
    
    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    service = OllamaChatService(ollama_url)
    
    try:
        await service.initialize()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
        status = await service.check_ollama_status()
        if status:
            print("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
        else:
            print("‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return False
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
        models = await service.get_models()
        if models:
            print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ ({len(models)}):")
            for model in models:
                size_mb = model.get('size', 0) / (1024 * 1024) if model.get('size') else 0
                print(f"  - {model['name']} ({size_mb:.1f} MB)")
        else:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
            print("üí° –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å: ollama pull llama2")
            return False
        
        # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
        if models:
            test_model = models[0]['name']
            print(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–æ–¥–µ–ª—å—é: {test_model}")
            
            test_prompt = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
            response = await service.generate_response(test_model, test_prompt)
            
            if response and not response.startswith("–û—à–∏–±–∫–∞"):
                print(f"‚úÖ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—Å–ø–µ—à–µ–Ω")
                print(f"üìù –û—Ç–≤–µ—Ç: {response[:100]}...")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {response}")
                return False
        
        print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False
    
    finally:
        if service.http_session:
            await service.http_session.close()


async def test_streaming():
    """–¢–µ—Å—Ç –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    print("\nüåä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
    
    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    service = OllamaChatService(ollama_url)
    
    try:
        await service.initialize()
        
        models = await service.get_models()
        if not models:
            print("‚ö†Ô∏è –ù–µ—Ç –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            return False
        
        test_model = models[0]['name']
        test_prompt = "–†–∞—Å—Å–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ –∫–æ—Ç–∞"
        
        print(f"üìù –ü—Ä–æ–º–ø—Ç: {test_prompt}")
        print("üì§ –ü–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç:")
        
        full_response = ""
        async for chunk in service.stream_response(test_model, test_prompt):
            full_response += chunk
            print(chunk, end='', flush=True)
        
        print(f"\n‚úÖ –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ({len(full_response)} —Å–∏–º–≤–æ–ª–æ–≤)")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False
    
    finally:
        if service.http_session:
            await service.http_session.close()


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("=" * 50)
    print("ü§ñ –¢–ï–°–¢ –í–ï–ë-–ß–ê–¢–ê –° OLLAMA")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    print(f"üîó Ollama URL: {ollama_url}")
    
    # –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    connection_ok = await test_ollama_connection()
    
    if connection_ok:
        # –¢–µ—Å—Ç –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        await test_streaming()
    
    print("\n" + "=" * 50)
    if connection_ok:
        print("üéâ –í–ï–ë-–ß–ê–¢ –ì–û–¢–û–í –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
        print("üåê –û—Ç–∫—Ä–æ–π—Ç–µ: http://localhost:8501")
    else:
        print("‚ùå –¢–†–ï–ë–£–ï–¢–°–Ø –ù–ê–°–¢–†–û–ô–ö–ê")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
        print("üí° –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å: ollama pull llama2")
    print("=" * 50)


if __name__ == "__main__":
    asyncio.run(main())