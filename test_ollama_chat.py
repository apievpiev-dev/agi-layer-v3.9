#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–±-—á–∞—Ç–∞ —Å Ollama –±–µ–∑ Docker
"""

import asyncio
import aiohttp
import json
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent))

from services.ollama_web_chat import OllamaWebChat


async def test_ollama_connection():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama...")
    
    try:
        async with aiohttp.ClientSession() as session:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama
            async with session.get("http://localhost:11434/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    print(f"‚úÖ Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç! –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {models}")
                    return True
                else:
                    print(f"‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Å—Ç–∞—Ç—É—Å: {response.status})")
                    return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
        print("\nüí° –†–µ—à–µ–Ω–∏—è:")
        print("1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: https://ollama.ai/download")
        print("2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama: ollama serve")
        print("3. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å: ollama pull llama2")
        return False


async def test_web_chat():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–±-—á–∞—Ç–∞"""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–±-—á–∞—Ç–∞...")
    
    config = {
        'ollama_host': 'localhost',
        'ollama_port': 11434
    }
    
    try:
        chat_ui = OllamaWebChat(config)
        await chat_ui.initialize()
        
        # –¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        models = await chat_ui._get_available_models()
        print(f"‚úÖ –í–µ–±-—á–∞—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ú–æ–¥–µ–ª–∏: {models}")
        
        # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª–∏)
        if models:
            test_prompt = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
            print(f"üìù –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞: '{test_prompt}'")
            
            response = await chat_ui._generate_single_response(test_prompt, models[0])
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: {response[:100]}...")
        
        await chat_ui.cleanup()
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–±-—á–∞—Ç–∞: {e}")
        return False


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Ollama Chat Web Interface")
    print("=" * 50)
    
    # –¢–µ—Å—Ç 1: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama
    ollama_ok = await test_ollama_connection()
    
    if ollama_ok:
        # –¢–µ—Å—Ç 2: –í–µ–±-—á–∞—Ç
        chat_ok = await test_web_chat()
        
        if chat_ok:
            print("\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            print("\nüìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
            print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–µ–±-—á–∞—Ç: python3 start_ollama_chat.py")
            print("2. –û—Ç–∫—Ä–æ–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä: http://localhost:8502")
            print("3. –ù–∞—á–Ω–∏—Ç–µ –æ–±—â–µ–Ω–∏–µ —Å –ò–ò!")
        else:
            print("\n‚ùå –¢–µ—Å—Ç –≤–µ–±-—á–∞—Ç–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω")
            sys.exit(1)
    else:
        print("\n‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É –∏ –∑–∞–ø—É—Å–∫ Ollama.")
        sys.exit(1)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)